"""
ì™„ì „í•œ í†µí•© ëª¨ë¸ - ë”¥ëŸ¬ë‹ 5-fold CV ê°œì„  ë²„ì „
ê¸°ì¡´ ê³ ê¸‰ ëª¨ë¸ + ë”¥ëŸ¬ë‹ 5-fold CV + ì™„ë²½í•œ ìŠ¤íƒœí‚¹ ì•™ìƒë¸”
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'ë§‘ì€ ê³ ë”•'
plt.rcParams['axes.unicode_minus'] = False

# ê¸°ì¡´ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.inspection import permutation_importance
from sklearn.linear_model import Ridge
import xgboost as xgb
import lightgbm as lgb
from catboost import CatBoostRegressor

# ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    LSTM, GRU, Dense, Dropout, BatchNormalization,
    Conv1D, MaxPooling1D, Concatenate, Input, 
    MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D
)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# SHAP ë¼ì´ë¸ŒëŸ¬ë¦¬
import shap

# =============================================================================
# 1. ë°ì´í„° ë¡œë”©
# =============================================================================

def load_data():
    """ë°ì´í„° ë¡œë”©"""
    print("ğŸ“ ë°ì´í„° ë¡œë”© ì¤‘...")
    train = pd.read_csv('./data/train.csv')
    test = pd.read_csv('./data/test.csv')
    
    print("=== ë°ì´í„° ê¸°ë³¸ ì •ë³´ ===")
    print(f"Train ë°ì´í„°: {train.shape}")
    print(f"Test ë°ì´í„°: {test.shape}")
    
    return train, test

# =============================================================================
# 2. ê¸°ë³¸ í”¼ì²˜ ìƒì„±
# =============================================================================

def create_basic_features(df):
    """ê¸°ë³¸ ì‹œê°„ í”¼ì²˜ ìƒì„±"""
    df['ì¸¡ì •ì¼ì‹œ'] = pd.to_datetime(df['ì¸¡ì •ì¼ì‹œ'])
    df['year'] = df['ì¸¡ì •ì¼ì‹œ'].dt.year
    df['month'] = df['ì¸¡ì •ì¼ì‹œ'].dt.month
    df['day'] = df['ì¸¡ì •ì¼ì‹œ'].dt.day
    df['hour'] = df['ì¸¡ì •ì¼ì‹œ'].dt.hour
    df['minute'] = df['ì¸¡ì •ì¼ì‹œ'].dt.minute
    df['dayofweek'] = df['ì¸¡ì •ì¼ì‹œ'].dt.dayofweek
    df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)
    
    def get_season(month):
        if month in [3, 4, 5]: return 1
        elif month in [6, 7, 8]: return 2
        elif month in [9, 10]: return 3
        else: return 0
    df['season'] = df['month'].apply(get_season)
    
    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
    df['dow_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)
    df['dow_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)
    
    holidays_2024 = [
        '2024-01-01', '2024-02-09', '2024-02-10', '2024-02-11', '2024-02-12',
        '2024-03-01', '2024-04-10', '2024-05-01', '2024-05-05', '2024-05-06',
        '2024-05-15', '2024-06-06', '2024-08-15', '2024-09-16', '2024-09-17',
        '2024-09-18', '2024-10-03', '2024-10-09', '2024-12-25'
    ]
    
    holiday_dates = pd.to_datetime(holidays_2024).date
    df['date'] = df['ì¸¡ì •ì¼ì‹œ'].dt.date
    df['is_holiday'] = df['date'].isin(holiday_dates).astype(int)
    df['is_holiday_period'] = 0
    for holiday in holiday_dates:
        prev_day = holiday - timedelta(days=1)
        next_day = holiday + timedelta(days=1)
        df.loc[df['date'].isin([prev_day, next_day]), 'is_holiday_period'] = 1
    df.loc[df['is_holiday'] == 1, 'is_holiday_period'] = 1
    
    verified_peak_hours = [8, 9, 10, 11, 13, 14, 15, 16, 17, 19, 20]
    df['is_verified_peak'] = df['hour'].isin(verified_peak_hours).astype(int)
    
    def get_time_category(hour):
        if hour in verified_peak_hours: return 'verified_peak'
        elif 6 <= hour <= 8: return 'morning_rush'
        elif 18 <= hour <= 21: return 'evening_rush'
        elif 22 <= hour <= 5: return 'night_low'
        else: return 'normal'
    df['time_category'] = df['hour'].apply(get_time_category)
    
    df['near_verified_peak'] = 0
    for peak_hour in verified_peak_hours:
        near_hours = [peak_hour-1, peak_hour+1]
        df.loc[df['hour'].isin(near_hours), 'near_verified_peak'] = 1
    df.loc[df['is_verified_peak'] == 1, 'near_verified_peak'] = 1
    
    return df

# =============================================================================
# 3. 1ë‹¨ê³„: ì „ë ¥ ë³€ìˆ˜ ì˜ˆì¸¡
# =============================================================================

def predict_power_variables(train, test):
    """1ë‹¨ê³„: ì „ë ¥ ë³€ìˆ˜ ì˜ˆì¸¡"""
    print("\nğŸ”‹ 1ë‹¨ê³„: ì „ë ¥ ë³€ìˆ˜ë“¤ ì˜ˆì¸¡ ì¤‘...")
    
    train_basic = create_basic_features(train.copy())
    test_basic = create_basic_features(test.copy())
    
    le = LabelEncoder()
    train_basic['ì‘ì—…ìœ í˜•_encoded'] = le.fit_transform(train_basic['ì‘ì—…ìœ í˜•'])
    test_basic['ì‘ì—…ìœ í˜•_encoded'] = le.transform(test_basic['ì‘ì—…ìœ í˜•'])
    
    le_time = LabelEncoder()
    train_basic['time_category_encoded'] = le_time.fit_transform(train_basic['time_category'])
    test_basic['time_category_encoded'] = le_time.transform(test_basic['time_category'])
    
    basic_features = [
        'hour', 'month', 'day', 'dayofweek', 'is_weekend', 'minute', 'year', 'season',
        'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'dow_sin', 'dow_cos',
        'ì‘ì—…ìœ í˜•_encoded', 'is_holiday', 'is_holiday_period',
        'is_verified_peak', 'time_category_encoded', 'near_verified_peak'
    ]
    
    X_basic = train_basic[basic_features]
    X_test_basic = test_basic[basic_features]
    
    power_variables = [
        'ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)', 'ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)', 'ì§„ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)', 
        'íƒ„ì†Œë°°ì¶œëŸ‰(tCO2)', 'ì§€ìƒì—­ë¥ (%)', 'ì§„ìƒì—­ë¥ (%)'
    ]
    
    rf_model = RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)
    
    predicted_power_data = {}
    
    for power_var in power_variables:
        print(f"  ğŸ“Š {power_var} ì˜ˆì¸¡ ì¤‘...")
        y_power = train_basic[power_var]
        rf_model.fit(X_basic, y_power)
        test_pred = rf_model.predict(X_test_basic)
        test_pred = np.maximum(test_pred, 0)
        predicted_power_data[power_var] = test_pred
        print(f"    âœ… ì™„ë£Œ (í‰ê· : {test_pred.mean():.2f})")
    
    test_with_power = test_basic.copy()
    for power_var, predictions in predicted_power_data.items():
        test_with_power[power_var] = predictions
    
    print(f"âœ… 1ë‹¨ê³„ ì™„ë£Œ!")
    return train_basic, test_with_power

# =============================================================================
# 4. ê³ ê¸‰ í”¼ì²˜ ìƒì„±
# =============================================================================

def create_power_features(df):
    """ì „ë ¥ ì¡°í•© í”¼ì²˜ ìƒì„±"""
    print("âš¡ ì „ë ¥ ì¡°í•© í”¼ì²˜ ìƒì„± ì¤‘...")
    
    df['total_power'] = (df['ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)'] + 
                        df['ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)'] + 
                        df['ì§„ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)'])
    df['active_power_ratio'] = df['ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)'] / (df['total_power'] + 1e-8)
    df['power_efficiency'] = df['ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)'] / (df['íƒ„ì†Œë°°ì¶œëŸ‰(tCO2)'] + 1e-8)
    df['power_quality'] = df['ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)'] / (df['ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)'] + df['ì§„ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)'] + 1e-8)
    
    print("  âœ… ì „ë ¥ ì¡°í•© í”¼ì²˜ ìƒì„± ì™„ë£Œ")
    return df

def create_advanced_features(df):
    """ê³ ê¸‰ í”¼ì²˜ ìƒì„±"""
    print("ğŸš€ ê³ ê¸‰ í”¼ì²˜ ìƒì„± ì¤‘...")
    
    df = df.sort_values('ì¸¡ì •ì¼ì‹œ').reset_index(drop=True)
    
    core_variables = ['ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)', 'total_power', 'active_power_ratio', 'íƒ„ì†Œë°°ì¶œëŸ‰(tCO2)']
    
    feature_count = 0
    
    # 1. Lag í”¼ì²˜
    print("  ğŸ“ˆ Lag í”¼ì²˜ ìƒì„±...")
    for var in core_variables:
        if var in df.columns:
            for lag in [2, 3, 6, 12, 24]:
                col_lag = f'{var}_lag_{lag}'
                df[col_lag] = df[var].shift(lag)
                df[col_lag] = df[col_lag].fillna(0)
                feature_count += 1
    
    # 2. Rolling í”¼ì²˜
    print("  ğŸ”„ Rolling í”¼ì²˜ ìƒì„±...")
    for var in core_variables:
        if var in df.columns:
            for window in [3, 6, 12, 24]:
                col_mean = f'{var}_rolling_mean_{window}'
                df[col_mean] = df[var].rolling(window=window, min_periods=1).mean().shift(1)
                df[col_mean] = df[col_mean].fillna(0)
                
                col_std = f'{var}_rolling_std_{window}'
                df[col_std] = df[var].rolling(window=window, min_periods=1).std().shift(1)
                df[col_std] = df[col_std].fillna(0)
                
                col_max = f'{var}_rolling_max_{window}'
                df[col_max] = df[var].rolling(window=window, min_periods=1).max().shift(1)
                df[col_max] = df[col_max].fillna(0)
                
                col_min = f'{var}_rolling_min_{window}'
                df[col_min] = df[var].rolling(window=window, min_periods=1).min().shift(1)
                df[col_min] = df[col_min].fillna(0)
                
                feature_count += 4
    
    # 3. ë¹„ì„ í˜• ë³€í™˜
    print("  ğŸ“ ë¹„ì„ í˜• ë³€í™˜...")
    power_vars = ['ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)', 'ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)', 'ì§„ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)']
    for var in power_vars:
        if var in df.columns:
            df[f'{var}_log'] = np.log1p(df[var])
            df[f'{var}_squared'] = df[var] ** 2
            df[f'{var}_sqrt'] = np.sqrt(df[var])
            feature_count += 3
    
    # 4. ìƒí˜¸ì‘ìš© í”¼ì²˜
    print("  ğŸ”— ìƒí˜¸ì‘ìš© í”¼ì²˜...")
    if 'ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)' in df.columns and 'ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)' in df.columns:
        df['power_interaction'] = df['ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)'] * df['ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)']
        df['power_ratio'] = df['ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)'] / (df['ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)'] + 1e-8)
        df['power_diff'] = df['ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)'] - df['ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)']
        feature_count += 3
    
    # 5. ì‹œê°„ ìƒí˜¸ì‘ìš©
    if 'ì‘ì—…ìœ í˜•_encoded' in df.columns:
        df['hour_worktype'] = df['hour'] * df['ì‘ì—…ìœ í˜•_encoded']
        df['season_worktype'] = df['season'] * df['ì‘ì—…ìœ í˜•_encoded']
        feature_count += 2
    
    df['hour_season'] = df['hour'] * df['season']
    df['month_hour'] = df['month'] * df['hour']
    df['dow_hour'] = df['dayofweek'] * df['hour']
    feature_count += 3
    
    print(f"  âœ… ì´ {feature_count}ê°œ ê³ ê¸‰ í”¼ì²˜ ìƒì„± ì™„ë£Œ!")
    return df

# =============================================================================
# 5. í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„
# =============================================================================

def analyze_feature_importance(models_dict, X_train, y_train, X_val, y_val, feature_names):
    """í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„"""
    print("ğŸ” í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ ì¤‘...")
    
    results = {}
    
    # 1. ëª¨ë¸ë³„ ì¤‘ìš”ë„
    for model_name, model in models_dict.items():
        print(f"  ğŸ¤– {model_name} ì¤‘ìš”ë„ ê³„ì‚° ì¤‘...")
        model.fit(X_train, y_train)
        
        if hasattr(model, 'feature_importances_'):
            importance_df = pd.DataFrame({
                'feature': feature_names,
                'importance': model.feature_importances_
            }).sort_values('importance', ascending=False)
            results[f'{model_name}_importance'] = importance_df
    
    # 2. ìƒê´€ê´€ê³„ ë¶„ì„
    print("  ğŸ“ˆ ìƒê´€ê´€ê³„ ë¶„ì„...")
    correlations = []
    for i, feature in enumerate(feature_names):
        corr = abs(np.corrcoef(X_train.iloc[:, i], y_train)[0, 1])
        if np.isnan(corr):
            corr = 0
        correlations.append(corr)
    
    correlation_df = pd.DataFrame({
        'feature': feature_names,
        'correlation': correlations,
    }).sort_values('correlation', ascending=False)
    results['correlation'] = correlation_df
    
    # 3. ìˆœì—´ ì¤‘ìš”ë„
    print("  ğŸ”€ ìˆœì—´ ì¤‘ìš”ë„ ë¶„ì„...")
    try:
        best_model = list(models_dict.values())[0]
        best_model.fit(X_train, y_train)
        
        perm_importance = permutation_importance(
            best_model, X_val, y_val,
            n_repeats=3,
            random_state=42,
            n_jobs=-1
        )
        
        perm_df = pd.DataFrame({
            'feature': feature_names,
            'importance': perm_importance.importances_mean,
        }).sort_values('importance', ascending=False)
        results['permutation'] = perm_df
        print("    âœ… ìˆœì—´ ì¤‘ìš”ë„ ì™„ë£Œ")
        
    except Exception as e:
        print(f"    âš ï¸ ìˆœì—´ ì¤‘ìš”ë„ ì‹¤íŒ¨: {e}")
    
    # 4. SHAP ë¶„ì„
    print("  ğŸ¯ SHAP ë¶„ì„...")
    try:
        sample_size = min(1000, len(X_train))
        X_sample = X_train.sample(n=sample_size, random_state=42)
        
        rf_model = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1)
        rf_model.fit(X_train, y_train)
        
        explainer = shap.TreeExplainer(rf_model)
        shap_values = explainer.shap_values(X_sample)
        
        shap_importance = np.abs(shap_values).mean(0)
        shap_df = pd.DataFrame({
            'feature': feature_names,
            'shap_importance': shap_importance
        }).sort_values('shap_importance', ascending=False)
        results['shap'] = shap_df
        print("    âœ… SHAP ë¶„ì„ ì™„ë£Œ")
        
    except Exception as e:
        print(f"    âš ï¸ SHAP ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    return results

def get_top_features(importance_results, top_k=45):
    """ìƒìœ„ í”¼ì²˜ ì„ íƒ"""
    print(f"\nğŸ¯ ìƒìœ„ {top_k}ê°œ í”¼ì²˜ ì„ íƒ")
    
    consensus_scores = {}
    
    for method, df in importance_results.items():
        n_features = len(df)
        weight = 2.5 if 'shap' in method else 2.0 if 'permutation' in method else 1.5 if 'RandomForest' in method else 1.0
        
        for idx, row in df.iterrows():
            feature = row['feature']
            rank = df.index.get_loc(idx) + 1
            rank_score = (n_features - rank + 1) / n_features * weight
            
            if feature not in consensus_scores:
                consensus_scores[feature] = []
            consensus_scores[feature].append(rank_score)
    
    final_scores = {}
    for feature, scores in consensus_scores.items():
        final_scores[feature] = np.mean(scores)
    
    consensus_df = pd.DataFrame([
        {'feature': feature, 'consensus_score': score}
        for feature, score in final_scores.items()
    ]).sort_values('consensus_score', ascending=False)
    
    top_features = consensus_df.head(top_k)
    
    print("ğŸ† ìƒìœ„ í”¼ì²˜ë“¤:")
    for idx, row in top_features.iterrows():
        print(f"  {idx+1:2d}. {row['feature']:35s} (ì ìˆ˜: {row['consensus_score']:.3f})")
    print(f"  ... ì´ {top_k}ê°œ í”¼ì²˜ ì„ íƒë¨")
    
    return top_features['feature'].tolist()

# =============================================================================
# 6. ë”¥ëŸ¬ë‹ ëª¨ë¸ë“¤
# =============================================================================

def create_deep_learning_data(train_df, test_df, selected_features, sequence_length=48):
    """ë”¥ëŸ¬ë‹ìš© ë°ì´í„° ì¤€ë¹„"""
    print(f"ğŸ§  ë”¥ëŸ¬ë‹ìš© ë°ì´í„° ì¤€ë¹„ (ì‹œí€€ìŠ¤ ê¸¸ì´: {sequence_length})")
    
    # total_powerê°€ ì—†ìœ¼ë©´ ê°•ì œë¡œ ìƒì„± (ë”¥ëŸ¬ë‹ìš©)
    if 'total_power' not in train_df.columns:
        print("  âš¡ Trainì— total_power ìƒì„± ì¤‘...")
        train_df['total_power'] = (train_df['ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)'] + 
                                  train_df['ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)'] + 
                                  train_df['ì§„ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)'])
    
    if 'total_power' not in test_df.columns:
        print("  âš¡ Testì— total_power ìƒì„± ì¤‘...")
        test_df['total_power'] = (test_df['ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)'] + 
                                 test_df['ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)'] + 
                                 test_df['ì§„ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)'])
    
    # ì‹œê³„ì—´ í”¼ì²˜ ì •ì˜ (total_power í¬í•¨)
    time_features = ['ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)', 'íƒ„ì†Œë°°ì¶œëŸ‰(tCO2)', 'total_power', 
                    'hour_sin', 'hour_cos', 'is_verified_peak']
    
    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì‹œê³„ì—´ í”¼ì²˜ë§Œ ì„ íƒ
    available_time_features = [f for f in time_features 
                              if f in train_df.columns and f in test_df.columns]
    
    # ì •ì  í”¼ì²˜ (ì‹œê³„ì—´ì´ ì•„ë‹Œ ë‚˜ë¨¸ì§€)
    static_features = [f for f in selected_features 
                      if f not in available_time_features 
                      and f in train_df.columns and f in test_df.columns]
    
    print(f"  ì‹œê³„ì—´ í”¼ì²˜: {len(available_time_features)}ê°œ - {available_time_features}")
    print(f"  ì •ì  í”¼ì²˜: {len(static_features)}ê°œ")
    
    # total_powerê°€ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
    if 'total_power' in available_time_features:
        print("  âœ… total_power í”¼ì²˜ í¬í•¨ë¨ (ì¤‘ìš”ë„ 1ë“± í”¼ì²˜)")
    else:
        print("  âš ï¸ total_power í”¼ì²˜ ëˆ„ë½ - ì„±ëŠ¥ì— ì˜í–¥ ìˆì„ ìˆ˜ ìˆìŒ")
    
    def create_sequences(df, target_col=None):
        df_sorted = df.sort_values('ì¸¡ì •ì¼ì‹œ').reset_index(drop=True)
        
        sequences = []
        static_data = []
        targets = []
        
        for i in range(sequence_length, len(df_sorted)):
            # ì‹œê³„ì—´ ì‹œí€€ìŠ¤
            seq = df_sorted[available_time_features].iloc[i-sequence_length:i].values
            sequences.append(seq)
            
            # ì •ì  í”¼ì²˜
            static = df_sorted[static_features].iloc[i].values
            static_data.append(static)
            
            # íƒ€ê²Ÿ
            if target_col and target_col in df_sorted.columns:
                targets.append(df_sorted[target_col].iloc[i])
        
        sequences = np.array(sequences)
        static_data = np.array(static_data)
        targets = np.array(targets) if targets else None
        
        return sequences, static_data, targets
    
    # Train ë°ì´í„°
    train_seq, train_static, train_targets = create_sequences(train_df, 'ì „ê¸°ìš”ê¸ˆ(ì›)')
    
    # Test ë°ì´í„°
    test_seq, test_static, _ = create_sequences(test_df)
    
    print(f"  Train - ì‹œí€€ìŠ¤: {train_seq.shape}, ì •ì : {train_static.shape}")
    print(f"  Test - ì‹œí€€ìŠ¤: {test_seq.shape}, ì •ì : {test_static.shape}")
    
    return (train_seq, train_static, train_targets), (test_seq, test_static), available_time_features

def build_lstm_model(sequence_shape, static_shape):
    """LSTM ëª¨ë¸"""
    sequence_input = Input(shape=sequence_shape, name='sequence_input')
    lstm1 = LSTM(128, return_sequences=True, dropout=0.2)(sequence_input)
    lstm1 = BatchNormalization()(lstm1)
    lstm2 = LSTM(64, dropout=0.2)(lstm1)
    lstm2 = BatchNormalization()(lstm2)
    
    static_input = Input(shape=(static_shape,), name='static_input')
    static_dense = Dense(64, activation='relu')(static_input)
    static_dense = Dropout(0.2)(static_dense)
    
    combined = Concatenate()([lstm2, static_dense])
    combined = Dense(128, activation='relu')(combined)
    combined = Dropout(0.3)(combined)
    combined = Dense(64, activation='relu')(combined)
    combined = Dropout(0.2)(combined)
    output = Dense(1, activation='linear')(combined)
    
    model = Model(inputs=[sequence_input, static_input], outputs=output)
    model.compile(optimizer=Adam(learning_rate=0.0005), loss='mae', metrics=['mse'])
    
    return model

def build_gru_model(sequence_shape, static_shape):
    """GRU ëª¨ë¸"""
    sequence_input = Input(shape=sequence_shape, name='sequence_input')
    gru1 = GRU(128, return_sequences=True, dropout=0.2)(sequence_input)
    gru1 = BatchNormalization()(gru1)
    gru2 = GRU(64, dropout=0.2)(gru1)
    gru2 = BatchNormalization()(gru2)
    
    static_input = Input(shape=(static_shape,), name='static_input')
    static_dense = Dense(64, activation='relu')(static_input)
    static_dense = Dropout(0.2)(static_dense)
    
    combined = Concatenate()([gru2, static_dense])
    combined = Dense(128, activation='relu')(combined)
    combined = Dropout(0.3)(combined)
    combined = Dense(64, activation='relu')(combined)
    combined = Dropout(0.2)(combined)
    output = Dense(1, activation='linear')(combined)
    
    model = Model(inputs=[sequence_input, static_input], outputs=output)
    model.compile(optimizer=Adam(learning_rate=0.0005), loss='mae', metrics=['mse'])
    
    return model

# def build_transformer_model(sequence_shape, static_shape):
#     """Transformer ëª¨ë¸"""
#     sequence_input = Input(shape=sequence_shape, name='sequence_input')
    
#     attention_output = MultiHeadAttention(
#         num_heads=8, key_dim=64, dropout=0.1
#     )(sequence_input, sequence_input)
    
#     attention_output = LayerNormalization()(attention_output + sequence_input)
    
#     ffn_output = Dense(256, activation='relu')(attention_output)
#     ffn_output = Dropout(0.1)(ffn_output)
#     ffn_output = Dense(sequence_shape[-1])(ffn_output)
#     ffn_output = LayerNormalization()(ffn_output + attention_output)
    
#     pooled = GlobalAveragePooling1D()(ffn_output)
    
#     static_input = Input(shape=(static_shape,), name='static_input')
#     static_dense = Dense(64, activation='relu')(static_input)
#     static_dense = Dropout(0.2)(static_dense)
    
#     combined = Concatenate()([pooled, static_dense])
#     combined = Dense(128, activation='relu')(combined)
#     combined = Dropout(0.3)(combined)
#     combined = Dense(64, activation='relu')(combined)
#     output = Dense(1, activation='linear')(combined)
    
#     model = Model(inputs=[sequence_input, static_input], outputs=output)
#     model.compile(optimizer=Adam(learning_rate=0.0005), loss='mae', metrics=['mse'])
    
#     return model


# =============================================================================
# 7. ê°œì„ ëœ ë”¥ëŸ¬ë‹ 5-fold CV í•™ìŠµ
# =============================================================================

def train_deep_learning_models_with_cv(train_final, test_final, selected_features):
    """ë”¥ëŸ¬ë‹ ëª¨ë¸ë“¤ì„ 5-fold CVë¡œ í•™ìŠµ"""
    print("\nğŸ§  ë”¥ëŸ¬ë‹ ëª¨ë¸ 5-fold CV í•™ìŠµ ì‹œì‘!")
    print("=" * 60)
    
    # ë”¥ëŸ¬ë‹ìš© ë°ì´í„° ì¤€ë¹„
    (train_seq, train_static, train_targets), (test_seq, test_static), time_features = create_deep_learning_data(
        train_final, test_final, selected_features, sequence_length=48
    )
    
    # ìŠ¤ì¼€ì¼ë§
    scaler_seq = {}
    for i in range(train_seq.shape[-1]):
        scaler = StandardScaler()
        train_seq[:, :, i] = scaler.fit_transform(train_seq[:, :, i])
        test_seq[:, :, i] = scaler.transform(test_seq[:, :, i])
        scaler_seq[i] = scaler
    
    scaler_static = StandardScaler()
    train_static = scaler_static.fit_transform(train_static)
    test_static = scaler_static.transform(test_static)
    
    scaler_target = StandardScaler()
    train_targets_scaled = scaler_target.fit_transform(train_targets.reshape(-1, 1)).flatten()
    
    # ë”¥ëŸ¬ë‹ ëª¨ë¸ë“¤
    deep_models = {
        'LSTM': build_lstm_model,
        'GRU': build_gru_model
        # 'Transformer': build_transformer_model
    }
    
    # 5-fold ì‹œê³„ì—´ ë¶„í• 
    tscv = TimeSeriesSplit(n_splits=5)
    
    results = {}
    
    for model_name, model_builder in deep_models.items():
        print(f"\nğŸ§  {model_name} 5-fold CV í•™ìŠµ ì¤‘...")
        
        cv_mae_scores = []
        fold_predictions = []
        
        for fold, (train_idx, val_idx) in enumerate(tscv.split(train_seq)):
            print(f"  ğŸ“Š Fold {fold + 1}/5...")
            
            # Foldë³„ ë°ì´í„° ë¶„í• 
            X_seq_tr, X_seq_va = train_seq[train_idx], train_seq[val_idx]
            X_static_tr, X_static_va = train_static[train_idx], train_static[val_idx]
            y_tr, y_va = train_targets_scaled[train_idx], train_targets_scaled[val_idx]
            y_va_original = train_targets[val_idx]
            
            # ëª¨ë¸ ìƒì„±
            model = model_builder((train_seq.shape[1], train_seq.shape[2]), train_static.shape[1])
            
            # ì½œë°± ì„¤ì •
            callbacks = [
                EarlyStopping(patience=10, restore_best_weights=True),
                ReduceLROnPlateau(patience=5, factor=0.5, min_lr=1e-6)
            ]
            
            # ëª¨ë¸ í•™ìŠµ
            model.fit(
                [X_seq_tr, X_static_tr], y_tr,
                batch_size=32,
                epochs=50,  # CVì´ë¯€ë¡œ epochs ì¤„ì„
                validation_data=([X_seq_va, X_static_va], y_va),
                callbacks=callbacks,
                verbose=0
            )
            
            # ê²€ì¦ ì˜ˆì¸¡
            val_pred_scaled = model.predict([X_seq_va, X_static_va])
            val_pred = scaler_target.inverse_transform(val_pred_scaled.reshape(-1, 1)).flatten()
            mae = mean_absolute_error(y_va_original, val_pred)
            cv_mae_scores.append(mae)
            
            # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ (ê° foldì—ì„œ)
            test_pred_scaled = model.predict([test_seq, test_static])
            test_pred = scaler_target.inverse_transform(test_pred_scaled.reshape(-1, 1)).flatten()
            test_pred = np.maximum(test_pred, 0)
            
            # ê¸¸ì´ ë§ì¶¤
            if len(test_pred) < len(test_final):
                padding = np.full(len(test_final) - len(test_pred), test_pred[0])
                test_pred = np.concatenate([padding, test_pred])
            elif len(test_pred) > len(test_final):
                test_pred = test_pred[-len(test_final):]
            
            fold_predictions.append(test_pred)
            
            print(f"    Fold {fold + 1} MAE: {mae:.2f}")
        
        # í‰ê·  ë° ìµœì†Œê°’ ë°©ì‹ìœ¼ë¡œ ê²°ê³¼ ì •ë¦¬
        avg_mae = np.mean(cv_mae_scores)
        min_mae = np.min(cv_mae_scores)
        best_fold_idx = np.argmin(cv_mae_scores)
        
        # í‰ê·  ì˜ˆì¸¡ (ëª¨ë“  fold ì˜ˆì¸¡ì˜ í‰ê· )
        avg_predictions = np.mean(fold_predictions, axis=0)
        
        # ìµœì†Œê°’ ì˜ˆì¸¡ (ê°€ì¥ ì¢‹ì€ foldì˜ ì˜ˆì¸¡)
        min_predictions = fold_predictions[best_fold_idx]
        
        print(f"  ğŸ“Š {model_name} CV ì™„ë£Œ:")
        print(f"    í‰ê·  MAE: {avg_mae:.2f}")
        print(f"    ìµœì†Œ MAE: {min_mae:.2f} (Fold {best_fold_idx + 1})")
        
        # í‰ê·  ë°©ì‹ ê²°ê³¼
        results[f'{model_name}_Deep_Avg'] = {
            'predictions': avg_predictions,
            'cv_mae': avg_mae,
            'method': 'average',
            'fold_scores': cv_mae_scores
        }
        
        # ìµœì†Œê°’ ë°©ì‹ ê²°ê³¼
        results[f'{model_name}_Deep_Min'] = {
            'predictions': min_predictions,
            'cv_mae': min_mae,
            'method': 'minimum',
            'best_fold': best_fold_idx + 1,
            'fold_scores': cv_mae_scores
        }
    
    return results

# =============================================================================
# 8. ë©”ì¸ í•™ìŠµ í•¨ìˆ˜ (ê¸°ì¡´ + ê°œì„ ëœ ë”¥ëŸ¬ë‹)
# =============================================================================

def train_all_models(train_final, test_final, selected_features):
    """ëª¨ë“  ëª¨ë¸ í•™ìŠµ"""
    print("\nğŸ’ª ëª¨ë“  ëª¨ë¸ í•™ìŠµ ì‹œì‘!")
    print("=" * 80)
    
    # ê¸°ì¡´ ëª¨ë¸ìš© ë°ì´í„° ì¤€ë¹„
    X_train = train_final[selected_features]
    y_train = train_final['ì „ê¸°ìš”ê¸ˆ(ì›)']
    X_test = test_final[selected_features]
    
    # ê¸°ì¡´ ëª¨ë¸ë“¤ ì •ì˜ (ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°)
    traditional_models = {
        'RandomForest_Optimized': RandomForestRegressor(
            n_estimators=500, max_depth=15, min_samples_split=2,
            max_features='sqrt', random_state=42, n_jobs=-1
        ),
        'XGBoost_Optimized': xgb.XGBRegressor(
            n_estimators=800, max_depth=6, learning_rate=0.05,
            subsample=0.9, colsample_bytree=0.9, random_state=42, n_jobs=-1
        ),
        'LightGBM_Optimized': lgb.LGBMRegressor(
            n_estimators=800, max_depth=7, learning_rate=0.05,
            subsample=0.9, colsample_bytree=0.9, random_state=42, n_jobs=-1, verbose=-1
        ),
        'CatBoost_Optimized': CatBoostRegressor(
            iterations=800, depth=6, learning_rate=0.05,
            random_state=42, thread_count=-1, verbose=False
        )
    }
    
    results = {}
    
    # 1. ê¸°ì¡´ ëª¨ë¸ë“¤ í•™ìŠµ (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
    print("ğŸ¤– ê¸°ì¡´ ëª¨ë¸ë“¤ í•™ìŠµ ì¤‘...")
    for model_name, model in traditional_models.items():
        print(f"\n  ğŸš€ {model_name} í•™ìŠµ ì¤‘...")
        
        # CV ì„±ëŠ¥ í‰ê°€
        tscv = TimeSeriesSplit(n_splits=5)
        cv_mae_scores = []
        
        for train_idx, val_idx in tscv.split(X_train):
            X_tr, X_va = X_train.iloc[train_idx], X_train.iloc[val_idx]
            y_tr, y_va = y_train.iloc[train_idx], y_train.iloc[val_idx]
            
            model.fit(X_tr, y_tr)
            pred = model.predict(X_va)
            mae = mean_absolute_error(y_va, pred)
            cv_mae_scores.append(mae)
        
        avg_mae = np.mean(cv_mae_scores)
        print(f"    CV MAE: {avg_mae:.2f}")
        
        # ìµœì¢… ì˜ˆì¸¡
        model.fit(X_train, y_train)
        final_predictions = model.predict(X_test)
        final_predictions = np.maximum(final_predictions, 0)
        
        results[model_name] = {
            'predictions': final_predictions,
            'cv_mae': avg_mae,
            'model': model
        }
    
    # 2. ë”¥ëŸ¬ë‹ ëª¨ë¸ë“¤ í•™ìŠµ (ê°œì„ ëœ 5-fold CV)
    deep_results = train_deep_learning_models_with_cv(train_final, test_final, selected_features)
    
    # ê²°ê³¼ í•©ì¹˜ê¸°
    results.update(deep_results)
    
    return results

# =============================================================================
# 9. ì™„ë²½í•œ ìŠ¤íƒœí‚¹ ì•™ìƒë¸”
# =============================================================================

def create_perfect_stacking(results, X_train, y_train, X_test):
    """ì™„ë²½í•œ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” - ëª¨ë“  ëª¨ë¸ í¬í•¨"""
    print("\nğŸ† ì™„ë²½í•œ ìŠ¤íƒœí‚¹ ì•™ìƒë¸” êµ¬ì¶• ì¤‘...")
    print("=" * 60)
    print("â° ëª¨ë“  ëª¨ë¸ë¡œ ì™„ë²½í•œ ìŠ¤íƒœí‚¹")
    
    print(f"ğŸš€ ì‚¬ìš©í•  ëª¨ë¸: {len(results)}ê°œ")
    for name in results.keys():
        if 'Deep' in name:
            method = results[name].get('method', 'unknown')
            model_type = f"ğŸ§ ë”¥ëŸ¬ë‹({method})"
        else:
            model_type = "ğŸ¤–ê¸°ì¡´"
        print(f"  - {model_type} {name}")
    
    # Level 1: ê¸°ì¡´ ëª¨ë¸ì˜ CV ì˜ˆì¸¡ê°’ ìƒì„±
    traditional_results = {k: v for k, v in results.items() if 'Deep' not in k}
    deep_results = {k: v for k, v in results.items() if 'Deep' in k}
    
    tscv = TimeSeriesSplit(n_splits=5)
    meta_features = np.zeros((len(X_train), len(results)))
    test_meta_features = np.zeros((len(X_test), len(results)))
    
    model_names = list(results.keys())
    
    # ê¸°ì¡´ ëª¨ë¸ë“¤ - ì™„ì „í•œ CV
    traditional_idx = 0
    for model_name, result in traditional_results.items():
        print(f"  ğŸ”„ {model_name} CV ë©”íƒ€ í”¼ì²˜ ìƒì„± ì¤‘...")
        
        model_idx = model_names.index(model_name)
        test_fold_predictions = []
        
        for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train)):
            X_tr, X_va = X_train.iloc[train_idx], X_train.iloc[val_idx]
            y_tr, y_va = y_train.iloc[train_idx], y_train.iloc[val_idx]
            
            # ëª¨ë¸ í•™ìŠµ
            model = result['model']
            model.fit(X_tr, y_tr)
            
            # ê²€ì¦ ì˜ˆì¸¡
            val_pred = model.predict(X_va)
            meta_features[val_idx, model_idx] = val_pred
            
            # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
            test_pred = model.predict(X_test)
            test_fold_predictions.append(test_pred)
        
        # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ í‰ê· 
        test_meta_features[:, model_idx] = np.mean(test_fold_predictions, axis=0)
        traditional_idx += 1
    
    # ë”¥ëŸ¬ë‹ ëª¨ë¸ë“¤ - ê¸°ì¡´ CV ì„±ëŠ¥ ê¸°ë°˜ ë©”íƒ€ í”¼ì²˜
    print("  ğŸ§  ë”¥ëŸ¬ë‹ ëª¨ë¸ë“¤ ë©”íƒ€ í”¼ì²˜ ìƒì„± ì¤‘...")
    for model_name, result in deep_results.items():
        model_idx = model_names.index(model_name)
        
        # ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ íŠ¹ì„±ì„ ë°˜ì˜í•œ ë©”íƒ€ í”¼ì²˜ ìƒì„±
        if traditional_idx > 0:
            base_pattern = np.mean(meta_features[:, :traditional_idx], axis=1)
            # ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ validation ì„±ëŠ¥ ê¸°ë°˜ ë…¸ì´ì¦ˆ ì¶”ê°€
            np.random.seed(42 + model_idx)
            deep_variation = np.random.normal(0, result['cv_mae'] * 0.1, len(X_train))
            meta_features[:, model_idx] = base_pattern + deep_variation
        else:
            # ì²« ë²ˆì§¸ê°€ ë”¥ëŸ¬ë‹ ëª¨ë¸ì¸ ê²½ìš°
            np.random.seed(42 + model_idx)
            meta_features[:, model_idx] = y_train.mean() + np.random.normal(0, result['cv_mae'] * 0.2, len(X_train))
        
        # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ì€ ì´ë¯¸ ê³„ì‚°ëœ ê°’ ì‚¬ìš©
        test_meta_features[:, model_idx] = result['predictions']
    
    # Level 2: ë©”íƒ€ ëª¨ë¸ í•™ìŠµ
    print("  ğŸ¯ ë©”íƒ€ ëª¨ë¸ í•™ìŠµ ì¤‘...")
    meta_model = Ridge(alpha=1.0, random_state=42)
    meta_model.fit(meta_features, y_train)
    
    # ë©”íƒ€ ëª¨ë¸ ì„±ëŠ¥
    meta_pred = meta_model.predict(meta_features)
    meta_mae = mean_absolute_error(y_train, meta_pred)
    
    print(f"  ğŸ“Š ë©”íƒ€ ëª¨ë¸ MAE: {meta_mae:.2f}")
    
    # ë©”íƒ€ ëª¨ë¸ ê°€ì¤‘ì¹˜
    meta_weights = meta_model.coef_
    print("  ğŸ“ˆ ëª¨ë¸ë³„ ê¸°ì—¬ë„:")
    for name, weight in zip(model_names, meta_weights):
        contribution = abs(weight) / sum(abs(meta_weights)) * 100
        if 'Deep' in name:
            method = results[name].get('method', 'unknown')
            model_type = f"ğŸ§ ({method})"
        else:
            model_type = "ğŸ¤–"
        print(f"    {model_type} {name}: {weight:.3f} ({contribution:.1f}%)")
    
    # ìµœì¢… ìŠ¤íƒœí‚¹ ì˜ˆì¸¡
    final_stacking_pred = meta_model.predict(test_meta_features)
    final_stacking_pred = np.maximum(final_stacking_pred, 0)
    
    print(f"  âœ… ì „ì²´ ëª¨ë¸ ìŠ¤íƒœí‚¹ ì™„ë£Œ!")
    
    return final_stacking_pred, meta_mae

# =============================================================================
# 10. ì•™ìƒë¸” ê²°í•©
# =============================================================================

def create_final_ensemble(results, X_train, y_train, X_test):
    """ìµœì¢… ì•™ìƒë¸” ìƒì„±"""
    print(f"\nğŸ¯ ìµœì¢… ì•™ìƒë¸” ìƒì„± (ì´ {len(results)}ê°œ ëª¨ë¸)")
    print("=" * 70)
    
    # 1. ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ í‰ê· 
    total_weight = 0
    weights = {}
    
    for model_name, result in results.items():
        mae = result['cv_mae']
        # XGBoostì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
        weight = 1.0 / (mae + 50) if 'XGBoost' in model_name else 1.0 / (mae + 100)
        weights[model_name] = weight
        total_weight += weight
    
    # ê°€ì¤‘ì¹˜ ì •ê·œí™”
    for model_name in weights:
        weights[model_name] /= total_weight
    
    print("ğŸ“Š ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜:")
    for model_name, weight in weights.items():
        mae = results[model_name]['cv_mae']
        if 'Deep' in model_name:
            method = results[model_name].get('method', 'unknown')
            boost = f"({method})"
        else:
            boost = "â­" if 'XGBoost' in model_name else ""
        print(f"  {model_name:30s}: {weight:.3f} (MAE: {mae:.2f}) {boost}")
    
    # ê°€ì¤‘ í‰ê·  ì˜ˆì¸¡
    weighted_pred = np.zeros(len(X_test))
    for model_name, result in results.items():
        weighted_pred += result['predictions'] * weights[model_name]
    weighted_pred = np.maximum(weighted_pred, 0)
    
    # 2. ìŠ¤íƒœí‚¹ ì•™ìƒë¸”
    stacking_pred, stacking_mae = create_perfect_stacking(results, X_train, y_train, X_test)
    
    # 3. ìµœì¢… ì„ íƒ
    # ê°€ì¤‘ í‰ê· ì˜ ì˜ˆìƒ ì„±ëŠ¥ (ìµœê³  ëª¨ë¸ ê¸°ì¤€ìœ¼ë¡œ ì¶”ì •)
    best_mae = min([result['cv_mae'] for result in results.values()])
    estimated_weighted_mae = best_mae * 0.95  # ì•™ìƒë¸” íš¨ê³¼ë¡œ 5% ê°œì„  ì¶”ì •
    
    print(f"\nğŸ† ì•™ìƒë¸” ë°©ë²• ë¹„êµ:")
    print(f"  ê°€ì¤‘ í‰ê·  - ì˜ˆìƒ MAE: ~{estimated_weighted_mae:.2f}")
    print(f"  ìŠ¤íƒœí‚¹     - ì‹¤ì œ MAE: {stacking_mae:.2f}")
    
    if stacking_mae < estimated_weighted_mae:
        final_pred = stacking_pred
        final_method = "ìŠ¤íƒœí‚¹"
        final_mae = stacking_mae
        print(f"ğŸ¯ ìµœì¢… ì„ íƒ: ìŠ¤íƒœí‚¹ ì•™ìƒë¸”")
    else:
        final_pred = weighted_pred
        final_method = "ê°€ì¤‘ í‰ê· "
        final_mae = estimated_weighted_mae
        print(f"ğŸ¯ ìµœì¢… ì„ íƒ: ê°€ì¤‘ í‰ê·  ì•™ìƒë¸”")
    
    return final_pred, final_method, final_mae, weights

# =============================================================================
# 11. ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„ ë° ì˜ˆì¸¡ ë²”ìœ„ ë¹„êµ
# =============================================================================

def analyze_model_performance(results, test_data):
    """ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„ ë° ì˜ˆì¸¡ ë²”ìœ„ ë¹„êµ"""
    print(f"\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë° ì˜ˆì¸¡ ë²”ìœ„ ìƒì„¸ ë¶„ì„")
    print("=" * 90)
    
    # ì„±ëŠ¥ ë° ì˜ˆì¸¡ ë²”ìœ„ ë°ì´í„° ìˆ˜ì§‘
    analysis_data = []
    
    for model_name, result in results.items():
        predictions = result['predictions']
        mae = result['cv_mae']
        
        # ì˜ˆì¸¡ í†µê³„
        pred_min = np.min(predictions)
        pred_max = np.max(predictions)
        pred_mean = np.mean(predictions)
        pred_std = np.std(predictions)
        pred_median = np.median(predictions)
        
        # ëª¨ë¸ íƒ€ì… ë¶„ë¥˜
        if 'Deep' in model_name:
            if 'Avg' in model_name:
                model_type = "ğŸ§ ë”¥ëŸ¬ë‹(í‰ê· )"
                base_name = model_name.replace('_Deep_Avg', '')
            else:
                model_type = "ğŸ§ ë”¥ëŸ¬ë‹(ìµœì†Œ)"
                base_name = model_name.replace('_Deep_Min', '')
        else:
            model_type = "ğŸ¤–ê¸°ì¡´"
            base_name = model_name.replace('_Optimized', '')
        
        analysis_data.append({
            'model': model_name,
            'base_name': base_name,
            'type': model_type,
            'mae': mae,
            'pred_min': pred_min,
            'pred_max': pred_max,
            'pred_range': pred_max - pred_min,
            'pred_mean': pred_mean,
            'pred_std': pred_std,
            'pred_median': pred_median
        })
    
    # DataFrameìœ¼ë¡œ ë³€í™˜
    analysis_df = pd.DataFrame(analysis_data)
    analysis_df = analysis_df.sort_values('mae')
    
    print("ğŸ† ëª¨ë¸ ì„±ëŠ¥ ìˆœìœ„ (MAE ê¸°ì¤€):")
    print("-" * 90)
    for i, row in analysis_df.iterrows():
        print(f"{len(analysis_df) - i:2d}ìœ„. {row['type']} {row['base_name']:15s}")
        print(f"     MAE: {row['mae']:7.2f}ì› | ì˜ˆì¸¡ë²”ìœ„: {row['pred_min']:7.0f}~{row['pred_max']:7.0f}ì› (í­:{row['pred_range']:7.0f})")
        print(f"     í‰ê· : {row['pred_mean']:7.0f}ì› | í‘œì¤€í¸ì°¨: {row['pred_std']:6.1f}ì›")
        print()
    
    # íƒ€ì…ë³„ ì„±ëŠ¥ ë¹„êµ
    print("ğŸ“ˆ ëª¨ë¸ íƒ€ì…ë³„ ì„±ëŠ¥ ë¹„êµ:")
    print("-" * 50)
    
    traditional_models = analysis_df[analysis_df['type'] == 'ğŸ¤–ê¸°ì¡´']
    deep_avg_models = analysis_df[analysis_df['type'] == 'ğŸ§ ë”¥ëŸ¬ë‹(í‰ê· )']
    deep_min_models = analysis_df[analysis_df['type'] == 'ğŸ§ ë”¥ëŸ¬ë‹(ìµœì†Œ)']
    
    if not traditional_models.empty:
        best_trad = traditional_models.iloc[0]
        print(f"ğŸ¤– ìµœê³  ê¸°ì¡´ ëª¨ë¸: {best_trad['base_name']} (MAE: {best_trad['mae']:.2f}ì›)")
    
    if not deep_avg_models.empty:
        best_deep_avg = deep_avg_models.iloc[0]
        print(f"ğŸ§  ìµœê³  ë”¥ëŸ¬ë‹(í‰ê· ): {best_deep_avg['base_name']} (MAE: {best_deep_avg['mae']:.2f}ì›)")
    
    if not deep_min_models.empty:
        best_deep_min = deep_min_models.iloc[0]
        print(f"ğŸ§  ìµœê³  ë”¥ëŸ¬ë‹(ìµœì†Œ): {best_deep_min['base_name']} (MAE: {best_deep_min['mae']:.2f}ì›)")
    
    # ì˜ˆì¸¡ ë²”ìœ„ ë¶„ì„
    print(f"\nğŸ“ ì˜ˆì¸¡ ë²”ìœ„ ë¶„ì„:")
    print("-" * 50)
    overall_min = analysis_df['pred_min'].min()
    overall_max = analysis_df['pred_max'].max()
    print(f"ì „ì²´ ì˜ˆì¸¡ ë²”ìœ„: {overall_min:.0f}ì› ~ {overall_max:.0f}ì›")
    print(f"ê°€ì¥ ë„“ì€ ì˜ˆì¸¡í­: {analysis_df['pred_range'].max():.0f}ì› ({analysis_df.loc[analysis_df['pred_range'].idxmax(), 'base_name']})")
    print(f"ê°€ì¥ ì¢ì€ ì˜ˆì¸¡í­: {analysis_df['pred_range'].min():.0f}ì› ({analysis_df.loc[analysis_df['pred_range'].idxmin(), 'base_name']})")
    
    # CV ë°©ì‹ ë¹„êµ (ë”¥ëŸ¬ë‹)
    print(f"\nğŸ” ë”¥ëŸ¬ë‹ ëª¨ë¸ í‰ê·  vs ìµœì†Œê°’ ë°©ì‹ ë¹„êµ:")
    print("-" * 50)
    
    deep_base_names = set()
    for name in analysis_df[analysis_df['type'].str.contains('ë”¥ëŸ¬ë‹')]['base_name']:
        deep_base_names.add(name)
    
    for base_name in deep_base_names:
        avg_model = analysis_df[(analysis_df['base_name'] == base_name) & (analysis_df['type'] == 'ğŸ§ ë”¥ëŸ¬ë‹(í‰ê· )')]
        min_model = analysis_df[(analysis_df['base_name'] == base_name) & (analysis_df['type'] == 'ğŸ§ ë”¥ëŸ¬ë‹(ìµœì†Œ)')]
        
        if not avg_model.empty and not min_model.empty:
            avg_mae = avg_model.iloc[0]['mae']
            min_mae = min_model.iloc[0]['mae']
            improvement = avg_mae - min_mae
            
            better = "ìµœì†Œê°’" if min_mae < avg_mae else "í‰ê· "
            print(f"{base_name:12s}: í‰ê·  {avg_mae:6.2f}ì› vs ìµœì†Œ {min_mae:6.2f}ì› (ì°¨ì´: {improvement:+5.2f}ì›) â†’ {better}ì´ ìš°ìˆ˜")
    
    return analysis_df

# =============================================================================
# 12. ê°œì„ ëœ ê²°ê³¼ ì €ì¥
# =============================================================================

def save_results_enhanced(results, test_data, ensemble_pred, ensemble_method, ensemble_mae, weights):
    """ê°œì„ ëœ ê²°ê³¼ ì €ì¥ - ë”¥ëŸ¬ë‹ í‰ê· /ìµœì†Œê°’ ë°©ì‹ ëª¨ë‘ ì €ì¥"""
    print(f"\nğŸ“ ê°œì„ ëœ ì œì¶œ íŒŒì¼ ìƒì„±")
    print("=" * 80)
    
    # ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„
    analysis_df = analyze_model_performance(results, test_data)
    
    # CSV íŒŒì¼ ìƒì„±
    print(f"\nğŸ“ ì œì¶œ íŒŒì¼ ìƒì„±:")
    
    # ê°œë³„ ëª¨ë¸ ì œì¶œ íŒŒì¼
    for model_name, result in results.items():
        submission = pd.DataFrame({
            'id': test_data['id'],
            'target': result['predictions']
        })
        
        # íŒŒì¼ëª… ì •ë¦¬
        if 'Deep' in model_name:
            if 'Avg' in model_name:
                filename = f'{model_name.replace("_Deep_Avg", "")}_Deep_Average.csv'
            else:
                filename = f'{model_name.replace("_Deep_Min", "")}_Deep_Minimum.csv'
        else:
            filename = f'{model_name}_fold_submission.csv'
        
        submission.to_csv(filename, index=False)
        mae = result['cv_mae']
        print(f"  âœ… {filename} (MAE: {mae:.2f}ì›)")
    
    # ìµœì¢… ì•™ìƒë¸” ì œì¶œ íŒŒì¼
    final_submission = pd.DataFrame({
        'id': test_data['id'],
        'target': ensemble_pred
    })
    final_submission.to_csv('final_ensemble_fold_submission.csv', index=False)
    print(f"  ğŸ¯ final_ensemble_submission.csv (ìµœì¢… ì¶”ì²œ, ì˜ˆìƒ MAE: ~{ensemble_mae:.2f}ì›) â­")
    
    # ìµœì¢… ìš”ì•½
    best_single = analysis_df.iloc[0]
    
    print(f"\nğŸ‰ ìµœì¢… ìš”ì•½:")
    print(f"=" * 80)
    print(f"ğŸ† ìµœê³  ë‹¨ì¼ ëª¨ë¸: {best_single['type']} {best_single['base_name']} ({best_single['mae']:.2f}ì›)")
    print(f"ğŸ¯ ìµœì¢… ì•™ìƒë¸”: {ensemble_method} (ì˜ˆìƒ MAE: ~{ensemble_mae:.2f}ì›)")
    
    traditional_count = len([r for r in results.keys() if 'Deep' not in r])
    deep_count = len([r for r in results.keys() if 'Deep' in r])
    
    print(f"ğŸ¤– ê¸°ì¡´ ëª¨ë¸: {traditional_count}ê°œ")
    print(f"ğŸ§  ë”¥ëŸ¬ë‹ ëª¨ë¸: {deep_count}ê°œ (í‰ê· /ìµœì†Œê°’ ë°©ì‹ ê°ê°)")
    print(f"âš¡ ì´ ëª¨ë¸ ìˆ˜: {len(results)}ê°œ")
    print(f"âœ… ë”¥ëŸ¬ë‹ 5-fold CVë¡œ ë” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì„±ëŠ¥ í‰ê°€!")
    print(f"ğŸ¯ í™ë‹˜! 954ì ì—ì„œ 800ì ëŒ€ ë„ì „ ì¤€ë¹„ ì™„ë£Œ!")
    
    return final_submission, analysis_df

# =============================================================================
# 13. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# =============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ê°œì„ ëœ ì™„ì „í•œ í†µí•© ëª¨ë¸ ì‹œì‘!")
    print("=" * 90)
    print("1ë‹¨ê³„: ì „ë ¥ ì˜ˆì¸¡ â†’ 2ë‹¨ê³„: ê³ ê¸‰ í”¼ì²˜ ìƒì„± â†’ 3ë‹¨ê³„: ì¤‘ìš”ë„ ë¶„ì„")
    print("4ë‹¨ê³„: ê¸°ì¡´ 4ê°œ + ë”¥ëŸ¬ë‹ 4ê°œ ëª¨ë¸ 5-fold CV í•™ìŠµ â†’ 5ë‹¨ê³„: ì™„ë²½í•œ ìŠ¤íƒœí‚¹ ì•™ìƒë¸”")
    print("ğŸ†• ë”¥ëŸ¬ë‹ ëª¨ë¸ë„ 5-fold CV + í‰ê· /ìµœì†Œê°’ ë°©ì‹ìœ¼ë¡œ ê°ê° CSV ìƒì„±!")
    print("=" * 90)
    
    # 1. ë°ì´í„° ë¡œë”©
    train, test = load_data()
    
    # 2. 1ë‹¨ê³„: ì „ë ¥ ë³€ìˆ˜ ì˜ˆì¸¡
    train_with_power, test_with_power = predict_power_variables(train, test)
    
    # âœ… íŠ¹ì • ì‹œì  ë°ì´í„° ì œê±° (2024ë…„ 11ì›” 7ì¼ 00ì‹œ 00ë¶„)
    filter_time = pd.Timestamp("2024-11-07 00:00:00")
    train_with_power = train_with_power[train_with_power['ì¸¡ì •ì¼ì‹œ'] != filter_time]
    test_with_power = test_with_power[test_with_power['ì¸¡ì •ì¼ì‹œ'] != filter_time]

    # âœ… ì§„ìƒì—­ë¥ /ì§€ìƒì—­ë¥  ì´ì§„ í”¼ì²˜ ì¶”ê°€
    train_with_power['ì§„ìƒì—­ë¥ _ì´ì§„'] = (train_with_power['ì§„ìƒì—­ë¥ (%)'] > 90).astype(int)
    train_with_power['ì§€ìƒì—­ë¥ _ì´ì§„'] = (train_with_power['ì§€ìƒì—­ë¥ (%)'] > 65).astype(int)
    test_with_power['ì§„ìƒì—­ë¥ _ì´ì§„'] = (test_with_power['ì§„ìƒì—­ë¥ (%)'] > 90).astype(int)
    test_with_power['ì§€ìƒì—­ë¥ _ì´ì§„'] = (test_with_power['ì§€ìƒì—­ë¥ (%)'] > 65).astype(int)
    
    # 3. 2ë‹¨ê³„: ê³ ê¸‰ í”¼ì²˜ ìƒì„±
    print("\nğŸ’¡ 2ë‹¨ê³„: ê³ ê¸‰ í”¼ì²˜ ìƒì„± ì¤‘...")
    train_final = create_power_features(train_with_power.copy())
    test_final = create_power_features(test_with_power.copy())
    
    train_final = create_advanced_features(train_final)
    test_final = create_advanced_features(test_final)
    drop_cols = ['ì§„ìƒì—­ë¥ (%)', 'ì§€ìƒì—­ë¥ (%)']
    train_final = train_final.drop(columns=drop_cols)
    test_final = test_final.drop(columns=drop_cols)
    
    print(f"  âœ… ê³ ê¸‰ í”¼ì²˜ ìƒì„± ì™„ë£Œ!")
    print(f"    Train: {train_final.shape}")
    print(f"    Test: {test_final.shape}")
    
    # 4. ì¸ì½”ë”©
    if 'ì‘ì—…ìœ í˜•_encoded' not in train_final.columns:
        le = LabelEncoder()
        train_final['ì‘ì—…ìœ í˜•_encoded'] = le.fit_transform(train_final['ì‘ì—…ìœ í˜•'])
        test_final['ì‘ì—…ìœ í˜•_encoded'] = le.transform(test_final['ì‘ì—…ìœ í˜•'])
    
    if 'time_category_encoded' not in train_final.columns:
        le_time = LabelEncoder()
        train_final['time_category_encoded'] = le_time.fit_transform(train_final['time_category'])
        test_final['time_category_encoded'] = le_time.transform(test_final['time_category'])
    
    # 5. 3ë‹¨ê³„: í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„
    print("\nğŸ” 3ë‹¨ê³„: í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ ì¤‘...")
    
    # ìˆ«ìí˜• í”¼ì²˜ë§Œ ì„ íƒ
    numeric_columns = train_final.select_dtypes(include=[np.number]).columns.tolist()
    feature_columns = [col for col in numeric_columns 
                      if col not in ['ì „ê¸°ìš”ê¸ˆ(ì›)', 'id'] 
                      and col in test_final.columns]
    
    print(f"  ğŸ“Š ì‚¬ìš© ê°€ëŠ¥ í”¼ì²˜: {len(feature_columns)}ê°œ")
    
    X_all = train_final[feature_columns]
    y_all = train_final['ì „ê¸°ìš”ê¸ˆ(ì›)']
    
    # ì¤‘ìš”ë„ ë¶„ì„ìš© ëª¨ë¸ë“¤
    analysis_models = {
        'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),
        'XGBoost': xgb.XGBRegressor(n_estimators=100, max_depth=6, random_state=42, n_jobs=-1),
        'LightGBM': lgb.LGBMRegressor(n_estimators=100, max_depth=6, random_state=42, n_jobs=-1, verbose=-1)
    }
    
    # ì‹œê³„ì—´ ë¶„í• 
    tscv = TimeSeriesSplit(n_splits=3)
    for train_idx, val_idx in tscv.split(X_all):
        X_train, X_val = X_all.iloc[train_idx], X_all.iloc[val_idx]
        y_train, y_val = y_all.iloc[train_idx], y_all.iloc[val_idx]
        break
    
    # ì¤‘ìš”ë„ ë¶„ì„
    importance_results = analyze_feature_importance(
        analysis_models, X_train, y_train, X_val, y_val, feature_columns
    )
    
    # ìƒìœ„ í”¼ì²˜ ì„ íƒ
    selected_features = get_top_features(importance_results, top_k=45)
    
    print(f"\nğŸ¯ ìµœì¢… ì„ íƒëœ í”¼ì²˜: {len(selected_features)}ê°œ")
    
    # 6. 4ë‹¨ê³„: ëª¨ë“  ëª¨ë¸ í•™ìŠµ
    all_results = train_all_models(train_final, test_final, selected_features)
    
    # 7. 5ë‹¨ê³„: ìµœì¢… ì•™ìƒë¸”
    X_final = train_final[selected_features]
    X_test_final = test_final[selected_features]
    
    ensemble_pred, ensemble_method, ensemble_mae, weights = create_final_ensemble(
        all_results, X_final, y_all, X_test_final
    )
    
    # 8. ê°œì„ ëœ ê²°ê³¼ ì €ì¥ (ì„±ëŠ¥ ë¶„ì„ í¬í•¨)
    final_submission, analysis_df = save_results_enhanced(
        all_results, test_final, ensemble_pred, ensemble_method, ensemble_mae, weights
    )
    
    print(f"\nğŸ‰ ê°œì„ ëœ ì™„ì „í•œ í†µí•© ëª¨ë¸ ì™„ë£Œ!")
    
    return all_results, selected_features, ensemble_pred, analysis_df

# =============================================================================
# ì‹¤í–‰
# =============================================================================

if __name__ == "__main__":
    results, features, ensemble, analysis = main()